Запуск приложения
------------

1. Запуск из исходников (приложение доступно по адресу http://127.0.0.1:5000)
    ```
    pip install -r ./src/web/requirements.txt
    python ./src/web/server.py
    ```
1. Сборка и запуск докер-образа (приложение доступно по адресу http://localhost)
    ```
    docker build -f ./src/web/Dockerfile -t server_image ./src/web
    docker run -it --rm --name=server_container -p=80:80 server_image
    ```


Структура проекта
------------

```

├── README.md                  
├── data                         
│   ├── processed
│   │     ├── хакатон_2.xlsx            --  датасет после очистки и аугметации
│   │     ├── cleared.csv               --  датасет после очистки и аугметации, классы в виде списков
│   │     ├── cleared_one_class.csv     --  датасет после очистки и аугметации, оставлено 6 классов, multi-label убраны
│   │     ├── cleared_one_other.csv     --  датасет после очистки и аугметации, оставлено 7 классов (добавлен класс `другое`), multi-label убраны
│   │     └── deepseek.xlsx             --  датасет после классификации через DeepSeek
│   └── raw                             # The original, immutable data dump.
│
├── docs                         
│   └── dataset description.txt         -- описание исходного датасета
│
├── notebooks                           # Jupyter notebooks.
│   ├── EDA.ipynb                       -- EDA и предобработка датасета
│   ├── baseline_denis.ipynb            -- обучение модели на базовых трансформерах
│   ├── bert2.ipynb                     -- обучение базовой модели Bert c multi-label
│   ├── bert_one_class.ipynb            -- обучение базовой модели Bert на 6 классах
│   ├── bert_one_class_ru.ipynb         -- обучение русскоязычной модели Bert на 6 классах с оверсемплингом
│   └── bert_ru_other.ipynb             -- **Итоговая модель.** Обучение русскоязычной модели Bert на 7 классах с оверсемплингом на обработанном датасете
├── requirements.txt                    # The requirements file for reproducing the analysis environment.
└── src                                 # Source code for use in this project.
    ├── __init__.py                     # Makes src a Python module.
    └── web                             web-приложение для демонстрации
        ├── models                      # ML model engineering (a folder for each model).
        ├── static                      -- CSS-стили
        ├── templates                   -- HTML-templates  
        ├── Dockerfile                  -- Dockerfile для сборки образа приложения
        ├── requirements.txt            # The requirements file for reproducing the analysis environment.
        ├── server.py                   -- серверная часть (на Flask)
        └── uwsg.ini                    -- конфигурация UWSG
              
```

Ход работ
------------

1. Для первоначальной разметки предоставленного датасета воспользовались DeepSeek. Затем дополнительно по ключевым словам расставили метки недостающие.
1. Часть команды занялась EDA и дальнейшей обработкой датасета, а часть обучением моделей.
1. Обучили модель на базовых трансформерах (```./notebooks/baseline_denis.ipynb```). Получили F1 Score (micro) = 0,598. 
1. Обучили базовую многоязычную Bert (bert-base-multilingual-cased) на датасете и в режиме multi-label (```./notebooks/bert2.ipynb```). Получили F1 Score (micro) = 0,61. Низкую оценку связали с низким качеством подготовки датасета, а также решили попробовать обучать модель только на данных с одним классом.
1. Обучили базовую многоязычную Bert (bert-base-multilingual-cased) в режиме multi-class (не multi-label) на датасете, очищенном от данных без класса или с несколькими классами (```./notebooks/bert_one_class.ipynb```). Получили F1 Score (micro) = 0,66. Также заметили явную переобученность модели в пользу классов с большим количеством данных.
1. Обучили русскоязычную Bert (DeepPavlov/rubert-base-cased) в режиме multi-class на датасете, очищенном от данных без класса или с несколькими классами c дополнительно проведенным оверсемплингом для выравнивания данных по классам(```./notebooks/bert_one_class_ru.ipynb```). Получили F1 Score (micro) = 0,74. Теперь для улучшения ннужен был готовый датасет. Заметили, что данная модель обязательно выбирает один из представленных в датасете для обучения классов, что противоречило условию о том, что может не подойти ни один класс.
1. При обработке датасета провели объединение столбцов с текстами. Провели анализ частоты слов по классам, почистили от предлогов и стоп-слов. С помощью Yandex-api провели аугментацию - замену синонимами и удвоили таким образом датасет. Для дальнейшего обучения выбранной модели убрали сэмплы с двумя и более метками, а для сэмплов без метки добавили новую - `другое`.
1. Обучили русскоязычную Bert (DeepPavlov/rubert-base-cased) в режиме multi-class на подготовленном на предыдущем шаге датасете (```./notebooks/bert_one_class_ru.ipynb```). Получили F1 Score = 0,88.
1. Создали вэб-приложение на Flask и подготовили Dockerfile.
1. Подготовили презентацию.


Как можно улучшить результат
------------

1. Поиск или сбор большего по объему датасета.
1. Изменение гиперпараметров при обучении модели.