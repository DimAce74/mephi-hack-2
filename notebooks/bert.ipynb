{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee76e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.2.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->iterative-stratification) (2.4.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->iterative-stratification) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->iterative-stratification) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->iterative-stratification) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->iterative-stratification) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->iterative-stratification) (2024.2.0)\n",
      "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 10:36:59.225091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746182219.653352      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746182219.778539      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e2b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5700 entries, 0 to 5699\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5700 non-null   object\n",
      " 1   class   5700 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 89.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/cleared-new/cleared_new.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb20524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].apply(lambda x: ast.literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec11574a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>—Ç–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –¥–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...</td>\n",
       "      <td>[—Ä–µ–∫–ª–∞–º–∞, –ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚≠êÔ∏è  –∫–Ω–æ–ø–∫–∞: ‚≠êÔ∏èstart‚≠êÔ∏è(https://t.me/major/start...</td>\n",
       "      <td>[—Å–æ—Ü—Å–µ—Ç–∏]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –≤ –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...</td>\n",
       "      <td>[—Å–æ—Ü—Å–µ—Ç–∏]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>—Ç–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...</td>\n",
       "      <td>[—Å–æ—Ü—Å–µ—Ç–∏]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>—É –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...</td>\n",
       "      <td>[–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                    class  \\\n",
       "0  —Ç–≤–æ–π –ª—É—á—à–∏–π —Å–µ–∫—Å —Å–ø—Ä—è—Ç–∞–Ω –∑–¥–µ—Å—å üîû  –¥–µ–ª—é—Å—å –∫–∞–Ω–∞–ª...  [—Ä–µ–∫–ª–∞–º–∞, –ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å]   \n",
       "1  ‚≠êÔ∏è  –∫–Ω–æ–ø–∫–∞: ‚≠êÔ∏èstart‚≠êÔ∏è(https://t.me/major/start...                [—Å–æ—Ü—Å–µ—Ç–∏]   \n",
       "2  –∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≥–¥–µ? –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –≤ –º–æ–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤...                [—Å–æ—Ü—Å–µ—Ç–∏]   \n",
       "3  —Ç–µ–º –≤—Ä–µ–º–µ–Ω–µ–º –º–æ—è –∞–≤—Ç–æ—Ä—Å–∫–∞—è —Ç–µ–ª–µ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –≤ —Ç...                [—Å–æ—Ü—Å–µ—Ç–∏]   \n",
       "4  —É –º–µ–Ω—è –µ—Å—Ç—å –¥–≤–æ—é—Ä–æ–¥–Ω–∞—è —Å–µ—Å—Ç—Ä–∞, —É –Ω–µ–µ –µ—Å—Ç—å —Å—ã–Ω ...           [–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å]   \n",
       "\n",
       "               labels  \n",
       "0  [1, 0, 1, 0, 0, 0]  \n",
       "1  [0, 0, 0, 1, 0, 0]  \n",
       "2  [0, 0, 0, 1, 0, 0]  \n",
       "3  [0, 0, 0, 1, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = ['–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å', '–ø–æ–ª–∏—Ç–∏–∫–∞', '—Ä–µ–∫–ª–∞–º–∞', '—Å–æ—Ü—Å–µ—Ç–∏', '—Å–ø–æ—Ä—Ç', '—é–º–æ—Ä']\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "sorted_labels = sorted(all_labels)\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∏–Ω–∞—Ä–∏–∑–∞—Ç–æ—Ä\n",
    "mlb = MultiLabelBinarizer(classes=sorted_labels)\n",
    "mlb.fit(sorted_labels)\n",
    "\n",
    "binary_matrix = mlb.transform(df['class'])\n",
    "df['labels'] = list(binary_matrix)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf68a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'–ª–∏—á–Ω–∞—è –∂–∏–∑–Ω—å': 0,\n",
       " '–ø–æ–ª–∏—Ç–∏–∫–∞': 1,\n",
       " '—Ä–µ–∫–ª–∞–º–∞': 2,\n",
       " '—Å–æ—Ü—Å–µ—Ç–∏': 3,\n",
       " '—Å–ø–æ—Ä—Ç': 4,\n",
       " '—é–º–æ—Ä': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = {label: idx for idx, label in enumerate(mlb.classes_)}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f816cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a16f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ (n_samples, n_classes)\n",
    "labels = np.array(pre_dataset[\"labels\"])\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ\n",
    "msss = MultilabelStratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idx, val_idx = next(msss.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "train_dataset = pre_dataset.select(train_idx)\n",
    "val_dataset = pre_dataset.select(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bad911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
      "0     426\n",
      "1     311\n",
      "2     840\n",
      "3     759\n",
      "4    1376\n",
      "5     873\n",
      "dtype: int64\n",
      "Validation —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
      "0    107\n",
      "1     78\n",
      "2    210\n",
      "3    190\n",
      "4    344\n",
      "5    218\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def print_label_distribution(pre_dataset, name):\n",
    "    labels = np.array(pre_dataset[\"labels\"])\n",
    "    label_counts = pd.DataFrame(labels).sum(axis=0)\n",
    "    print(f\"{name} —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\\n{label_counts}\")\n",
    "\n",
    "print_label_distribution(train_dataset, \"Train\")\n",
    "print_label_distribution(val_dataset, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e542ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset  # –∏–ª–∏ \"test\" –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ad94895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'class', 'labels'],\n",
      "        num_rows: 4560\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'class', 'labels'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c0c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c83be8017b64a218108ab6fc32a6671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48db2bffd61c44a4b064b3e5c87be82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ca577f494541d7aded0347dcedb345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf987ad40e946d380c9e95d034fbab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f60bc1654084cc5b6f69d9cff32bc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804352268c9e494486cb49364bf596a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8008033b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'class', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4560\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'class', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b002f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–∏–ø –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "–ö–ª—é—á–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: dict_keys(['train', 'test'])\n",
      "–¢–∏–ø –º–µ—Ç–æ–∫ –≤ train: <class 'list'>\n",
      "–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫: [[1, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"–¢–∏–ø –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:\", type(tokenized_datasets))\n",
    "print(\"–ö–ª—é—á–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:\", tokenized_datasets.keys())\n",
    "print(\"–¢–∏–ø –º–µ—Ç–æ–∫ –≤ train:\", type(tokenized_datasets[\"train\"][\"labels\"][0]))\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫:\", tokenized_datasets[\"train\"][\"labels\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2341f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9958c7e8da6a4207ad9cad746dad5d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff990823f34a47c49f5c8efb57ac5ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_float32(batch):\n",
    "    return {\n",
    "        'labels': [torch.tensor(x, dtype=torch.float32) for x in batch['labels']]\n",
    "    }\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_to_float32, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97966b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2c708c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d874c956a309441cad0476720e82a38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2950024048.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lambda x: {'labels': torch.tensor(x['labels'], dtype=torch.float32)},\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5a9209513d47a0805b3a449e6e9459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in tokenized_datasets:\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].map(\n",
    "        lambda x: {'labels': torch.tensor(x['labels'], dtype=torch.float32)},\n",
    "        batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adae7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–∏–ø –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: <class 'torch.Tensor'>\n",
      "–ü—Ä–∏–º–µ—Ä –º–µ—Ç–∫–∏: tensor([1., 0., 1., 0., 0., 0.])\n",
      "–§–æ—Ä–º–∞—Ç train: {'type': 'torch', 'format_kwargs': {'dtype': torch.float32}, 'columns': ['labels'], 'output_all_columns': False}\n",
      "–§–æ—Ä–º–∞—Ç test: {'type': 'torch', 'format_kwargs': {'dtype': torch.float32}, 'columns': ['labels'], 'output_all_columns': False}\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"–¢–∏–ø –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:\", type(tokenized_datasets[\"train\"][\"labels\"][0]))\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –º–µ—Ç–∫–∏:\", tokenized_datasets[\"train\"][\"labels\"][0])\n",
    "print(\"–§–æ—Ä–º–∞—Ç train:\", tokenized_datasets[\"train\"].format)\n",
    "print(\"–§–æ—Ä–º–∞—Ç test:\", tokenized_datasets[\"test\"].format)\n",
    "print(tokenized_datasets[\"train\"][\"labels\"][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6b6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"],\n",
    "    dtype=torch.float32,\n",
    "    output_all_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97dd77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].with_format(\n",
    "    \"torch\",\n",
    "    columns=[\"labels\"],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].with_format(\n",
    "    \"torch\",\n",
    "    columns=[\"labels\"],\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b488849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–∏–ø –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: <class 'torch.Tensor'>\n",
      "–ü—Ä–∏–º–µ—Ä –º–µ—Ç–∫–∏: tensor([1., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(\"–¢–∏–ø –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:\", type(tokenized_datasets[\"train\"][\"labels\"][0]))\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –º–µ—Ç–∫–∏:\", tokenized_datasets[\"train\"][\"labels\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "373a5d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels type: torch.FloatTensor\n",
      "Test labels type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels type:\", tokenized_datasets[\"train\"][0][\"labels\"].type())\n",
    "print(\"Test labels type:\", tokenized_datasets[\"test\"][0][\"labels\"].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d531bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§–æ—Ä–º–∞—Ç train: {'type': 'torch', 'format_kwargs': {'dtype': torch.float32}, 'columns': ['labels'], 'output_all_columns': False}\n",
      "–§–æ—Ä–º–∞—Ç test: {'type': 'torch', 'format_kwargs': {'dtype': torch.float32}, 'columns': ['labels'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"–§–æ—Ä–º–∞—Ç train:\", tokenized_datasets[\"train\"].format)\n",
    "print(\"–§–æ—Ä–º–∞—Ç test:\", tokenized_datasets[\"test\"].format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fea2258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'class', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4560\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'class', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af420b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15ee27a171c44e784c681fbda65a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", \n",
    "    num_labels=len(sorted_labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    log_level='info',\n",
    "    do_train=True,  # –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ!\n",
    "    do_eval=True,\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    fp16=True,  # –ï—Å–ª–∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç\n",
    "    report_to=[\"tensorboard\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e35ea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {'dtype': torch.float32},\n",
       " 'columns': ['labels'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'].format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "450c98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'labels'],  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ torch.Tensor\n",
    "    output_all_columns=True  # –û—Å—Ç–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –≤–∏–¥–µ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d474369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels type: torch.LongTensor\n",
      "Test labels type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels type:\", tokenized_datasets[\"train\"][0][\"labels\"].type())\n",
    "print(\"Test labels type:\", tokenized_datasets[\"test\"][0][\"labels\"].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ded37c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['input_ids', 'attention_mask', 'labels'],\n",
       " 'output_all_columns': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'].format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cfdcfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4600fde86544e18a7c9e814629bee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24f952ec05446ffb8c0742a2ea296a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda x: {\"labels\": x[\"labels\"].to(torch.float32)},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "527a8fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: class, text. If class, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4,560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Training with DataParallel so batch size has been adjusted to: 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,860\n",
      "  Number of trainable parameters = 177,858,054\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1713, in forward\n    loss = loss_fct(logits, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\", line 819, in forward\n    return F.binary_cross_entropy_with_logits(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 3628, in binary_cross_entropy_with_logits\n    return torch.binary_cross_entropy_with_logits(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: result type Float can't be cast to the desired output type Long\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/49973641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1713, in forward\n    loss = loss_fct(logits, labels)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\", line 819, in forward\n    return F.binary_cross_entropy_with_logits(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 3628, in binary_cross_entropy_with_logits\n    return torch.binary_cross_entropy_with_logits(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: result type Float can't be cast to the desired output type Long\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model.save_pretrained(\"./my_bert_classifier\")\n",
    "tokenizer.save_pretrained(\"./my_bert_classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
